prompt: |
  You are inside a Linux container without internet.

  Inputs (JSON) under /workdir/data/:
    - shipments.json : array of shipments with fields:
        expected_days (int), actual_days (int), quantity (int),
        hops (array of strings), destination (string)
    - capacities.json: array of node capacities with fields:
        node (string), daily_capacity (number)

  Goal: write a CSV at /workdir/sol.csv with EXACT header:
      node,delay_contribution_score,weighted_impact,normalized_impact

  Rules:
    1) Consider only delayed shipments where actual_days > expected_days.
       delay = actual_days - expected_days.
    2) Nodes per delayed shipment = unique (case-insensitive) of hops[] + destination,
       in that order (dedupe within a shipment).
    3) Capacity lookup is case-insensitive; preserve canonical casing from capacities.
    4) Default capacity = median of POSITIVE daily_capacity values (rounded to integer).
       If no positive capacities exist, use 1.0.
    5) Per node:
         DCS = sum(delay) / capacity
         WI  = DCS * sum(max(quantity,0)) across delayed shipments for that node
         NI  = WI / max(WI)    # if max(WI)=0 => NI=0.0
    6) Output formatting:
         - EXACT header as above
         - Sort rows by: NI desc, then WI desc, then node asc (case-insensitive)
         - All numeric fields MUST have exactly 4 decimals (e.g., 1.2300)
         - No extra spaces, no blank lines
metadata:
  difficulty: hard
  category: data-analytics
  tags:
    - data-engineering
    - analytics
    - supply-chain
    - rounding
    - sorting
  references: []
  time_limit: 300      
  memory_limit: 1024         
  max_agent_timeout_sec: 600
  expert_time_estimate_min: 25
  junior_time_estimate_min: 50