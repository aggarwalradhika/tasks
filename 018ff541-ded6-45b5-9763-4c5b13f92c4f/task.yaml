prompt: |
  You are given a single input file at /workdir/data/stream.jsonl.

  ## Accepted line formats (either form may appear; lines are independent)
  1) Object line (structured):
     {
       "batch_id": <int>,                # if missing or invalid, auto-assigned in ingest order
       "type": "data" | "retract" | "checkpoint" | "adjust_window",
       // if type == "data":
       "records": [
         {"key": "<str>", "value": <int>, "weight": <int, optional>=1, "timestamp": <int, optional>},
         ...
       ],
       "metadata": {"priority": <int, optional>=0, "source": "<str, optional>"}
       
       // if type == "retract":
       "target_batch_id": <int>,
       "selective": {"keys": [<str>, ...]}  # optional field; if present, only retract these keys
       
       // if type == "checkpoint":
       "checkpoint_id": <str>,
       "action": "save" | "restore"  # save current state or restore to saved checkpoint
       
       // if type == "adjust_window":
       "new_window_size": <int>  # dynamically change window size (min: 3, max: 20)
     }

  2) Array line (compact):
     [<int>, <int>, ...]
     - Interpreted as a "data" batch for a single synthetic key "A".
     - Each value in the array is taken with weight = 1.
     - A batch_id is auto-assigned for this line.

  3) CSV line (legacy format):
     key1:val1:weight1,key2:val2:weight2,...
     - Colon-separated key:value:weight tuples, comma-delimited
     - Weight is optional (defaults to 1)
     - Example: "A:10:2,B:20,C:30:5"

  ## Normalization & tolerance rules (applied to each line before processing)
  - Missing/invalid "batch_id" → auto-assign an increasing integer in ingest order (1, 2, 3, ...).
  - For "data" lines:
      * "records" not present or not a list → treated as empty.
      * Each record must be an object with "key" (string) and "value" (int); otherwise it is ignored.
      * "weight" is optional; defaults to 1. If non-integer or ≤ 0, coerce to 1.
      * "timestamp" is optional; if present and valid, used for time-decay calculation.
      * "metadata.priority" affects ordering when multiple batches have same batch_id (higher priority processed first).
  - For "retract" lines:
      * If "selective.keys" is present, only retract records for those specific keys
      * Otherwise, retract all records from target batch
  - Lines that cannot be parsed as JSON/CSV, or that do not match any line format, are ignored.
  - Lines starting with '//' and blank lines are ignored.

  ## Fixed parameters for this task
  - percentiles = [10, 25, 50, 75, 90, 95, 99]  # nearest-rank, weighted
  - initial_window_size_batches = 6             # can be changed dynamically via adjust_window
  - time_decay_factor = 0.95                    # if timestamps present, apply exponential decay
  - min_weight_threshold = 0.1                  # records with effective weight < this are excluded

  ## Advanced Semantics

  ### 1. Dynamic Window Sizing
  When type="adjust_window" is encountered:
    - Change the sliding window size to new_window_size (clamp between 3-20)
    - This affects all subsequent window calculations
    - Window size changes apply immediately to the current ingest

  ### 2. Checkpointing
  When type="checkpoint" with action="save":
    - Save the complete current state (all window data, window size, etc.) with checkpoint_id
    - Multiple checkpoints can exist simultaneously
  
  When type="checkpoint" with action="restore":
    - Restore to the saved checkpoint_id state
    - All data/retractions/adjustments after that checkpoint are discarded
    - If checkpoint_id doesn't exist, this is a no-op
    - After restore, continue processing from the next line

  ### 3. Selective Retractions
  For type="retract":
    - If "selective.keys" is present and is a list of strings, only remove contributions 
      from the target batch for those specific keys
    - Otherwise, remove all contributions from target batch (existing behavior)

  ### 4. Time-Based Weight Decay
  If any record has a "timestamp" field (integer, Unix epoch seconds):
    - Calculate time_delta = current_ingest_timestamp - record_timestamp
    - Apply exponential decay: effective_weight = original_weight * (decay_factor ^ time_delta_hours)
    - current_ingest_timestamp is derived from the timestamp of the most recent timestamped record
    - Round effective_weight to 2 decimal places; exclude if < min_weight_threshold

  ### 5. Priority-Based Processing
  If multiple batches have the same batch_id (edge case):
    - Sort by metadata.priority (descending) before processing
    - Higher priority batches are processed first

  ### 6. Conditional Aggregations
  For each ingest, in addition to GLOBAL and per-KEY rows, emit:
    - HIGH_WEIGHT: aggregation of all records where weight >= 5
    - LOW_WEIGHT: aggregation of all records where weight < 5
    - POSITIVE: aggregation of all records where value > 0
    - NEGATIVE: aggregation of all records where value <= 0

  ### 7. Moving Average Smoothing (Optional)
  For percentile calculations, if enabled via environment variable SMOOTH_PERCENTILES=true:
    - For each percentile, take weighted average of current and previous 2 ingest values
    - Weights: current=0.5, prev1=0.3, prev2=0.2
    - For first 2 ingests, use available history

  ## Percentiles (nearest-rank with weights and time decay)
  For a multiset of values with floating-point effective weights (after time decay):
    Let N = total effective weight. For percentile p, r = ceil(p/100 * N).
    Sort values ascending and return the first value where cumulative weight >= r.
    If N < min_weight_threshold for a scope, leave all percentile cells empty.
    Round percentile values to integers.

  ## Output
  Write /workdir/sol.csv after processing all ingests. For each ingest i (1..T), append rows in this order:
    1) GLOBAL row (scope="GLOBAL", key="")
    2) HIGH_WEIGHT row (scope="HIGH_WEIGHT", key="")
    3) LOW_WEIGHT row (scope="LOW_WEIGHT", key="")
    4) POSITIVE row (scope="POSITIVE", key="")
    5) NEGATIVE row (scope="NEGATIVE", key="")
    6) Per-KEY rows (scope="KEY", key="<key>"), sorted lexicographically
    
  Skip any scope where total effective weight < min_weight_threshold.

  ## CSV schema (strict column order)
  ingest_index,window_start_ingest,window_end_ingest,window_size,scope,key,p10,p25,p50,p75,p90,p95,p99

  ## Additional Complexity Notes
  - Handle nested checkpoints (checkpoint within checkpoint state)
  - Window adjustments persist through checkpoint restores
  - Timestamp decay calculations must handle missing timestamps gracefully
  - Empty conditional aggregations should be omitted from output
  - Priority ties are broken by ingest order
  - CSV legacy format must be normalized to standard object format
  - Malformed selective retraction keys are ignored

  ## Notes
  - Values may be negative. Empty DATA batches are allowed.
  - No network access. Deterministic output only.
  - The file must be written to /workdir/sol.csv with the exact header and ordering above.
  - All floating-point calculations must be deterministic (no random rounding)

metadata:
  difficulty: hard
  category: algorithms
  tags: [streaming, percentiles, sliding-window, retractions, weighted, csv, checkpointing, time-series]
  references: []

time_limit: 3600
memory_limit: 1024
max_agent_timeout_sec: 2400
expert_time_estimate_min: 180
junior_time_estimate_min: 360